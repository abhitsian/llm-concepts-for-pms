<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Task Length Doubled Every 7 Months</title>
  <link rel="stylesheet" href="../css/style.css">
</head>
<body>
  
  <div class="article-card">
    <div class="article-meta">602 words</div><h1>Task Length Doubled Every Seven Months</h1><div class="article-subtitle">Part 8 of Design Patterns of the AI Era</div><div class="divider">╌╌╌╌</div><hr />
<figure>
<a class="diagram-link" href="https://excalidraw.com/#json=f-Sugg4xYdXNSyfcPq8Go,GFdAXt4fE-U20dBFo3Zraw">View diagram: Task Length Doubling</a>
<figcaption aria-hidden="true">Task Length Doubling</figcaption>
</figure>
<p>METR, the AI evaluation nonprofit, tracked something more useful than standard benchmarks: the length &amp; complexity of real-world software tasks that AI coding tools could complete autonomously. Their finding was that this capability roughly doubled every seven months. The metric wasn’t accuracy on a fixed test set — it was the size of the problem an AI agent could handle end-to-end without human intervention.</p>
<p>This matters far more than benchmark scores cuz it maps directly to product planning decisions. A benchmark tells you a model scored 87% on HumanEval. The METR trajectory tells you that a task requiring four hours of human work today might be within AI capability in fourteen months. That’s a different kind of information entirely — it’s a planning input, not an evaluation metric.</p>
<p>The doubling wasn’t about the AI getting smarter at the same tasks. It was about the expanding frontier of task complexity the tools could handle reliably. In early 2024, AI coding agents could manage single-function implementations with clear specifications. By late 2024, they were handling multi-file changes with test generation. By mid-2025, they were completing features that involved understanding existing codebases, making coordinated changes across multiple components, &amp; validating their own work through test execution.</p>
<h2 id="the-roadmap-implication">The Roadmap Implication</h2>
<p>The seven-month doubling period creates a concrete framework for product decisions. Take any task in your current workflow that requires human effort. Estimate its complexity relative to what AI tools can handle today. Count the doublings until AI capability reaches that task’s complexity level. That gives you a rough timeline for when the task becomes automatable — not perfectly, but well enough to change how you staff &amp; plan around it.</p>
<p>Two doublings out — roughly fourteen months — covers a substantial expansion of capability. Tasks that currently require a junior developer spending a full day might fall within AI agent range. Three doublings out — about twenty-one months — pushes the frontier into tasks that currently take experienced developers multiple days, involving cross-system understanding &amp; nuanced architectural judgment.</p>
<p>This doesn’t mean those tasks will be fully automated on that timeline. The doubling measures what AI can handle at some acceptable reliability threshold, not what it can handle perfectly. But “good enough with human review” is often sufficient to reshape team structures &amp; product timelines.</p>
<h2 id="the-practical-question">The Practical Question</h2>
<p>The useful exercise for any product team is to catalog the tasks that make up your development &amp; operational workflow, rank them by complexity, &amp; identify which ones sit within one to two doublings of current AI capability. Those are the tasks where your process will change first. Not hypothetically, not in some distant future — within the next product planning cycle.</p>
<p>The tasks just beyond current AI capability are where the leverage is highest. They’re complex enough that full automation isn’t here yet, but close enough that human-AI collaboration on them yields disproportionate productivity gains. An AI agent that can handle 70% of a task autonomously &amp; flags the remaining 30% for human judgment turns a day of work into two hours of review.</p>
<p>The seven-month doubling also means that building products around the assumption that certain tasks will always require humans is risky. The capability frontier moves fast enough that “humans will always need to do X” has a short shelf life. Plan for the trajectory, not the current snapshot. Whatever AI coding tools can’t do today, ask whether that limitation survives two more doublings. Often the honest answer is that it probably doesn’t.</p><div class="end-mark">╌╌ end ╌╌</div>
  </div>
  <nav class="article-nav">
  <a href="31-dp-07-vibe-coding-and-the-skill-inversion.html">&larr; Vibe Coding & the Skill Inversion</a>
  <a class="nav-index" href="../index.html">Index</a>
  <a href="33-dp-09-the-permission-gradient.html">The Permission Gradient &rarr;</a>
</nav>
  
</body>
</html>