<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Three Ways to Customize an LLM</title>
  <link rel="stylesheet" href="../css/style.css">
</head>
<body>
  
  <div class="article-card">
    <div class="article-meta">945 words</div><h1>Three Ways to Customize an LLM</h1><div class="article-subtitle">Part 0e of LLM Concepts for PMs</div><div class="divider">╌╌╌╌</div><hr />
<figure>
<a class="diagram-link" href="https://excalidraw.com/#json=GQtF0GzHWIQDPSFuJOIr5,IyoHj9Sg1Tcg3DynFadNjw">View diagram: Three Ways to Customize an LLM</a>
<figcaption aria-hidden="true">Three Ways to Customize an LLM</figcaption>
</figure>
<p>You’ve got a foundation model that knows a lot about the world in general &amp; nothing about your business in particular. It doesn’t know your return policy, your product catalog, your brand voice, or the twelve edge cases your support team handles every day. The model is generic. Your product can’t be. So you need to close the gap — &amp; there are exactly three ways to do it, roughly ordered by cost &amp; effort: prompt engineering, retrieval-augmented generation, &amp; fine-tuning. Understanding when to use which one is one of the highest-leverage decisions you’ll make as a PM.</p>
<p>Let’s make this concrete. Say you’re building a customer support bot for an e-commerce company. Customers ask about orders, returns, sizing, shipping times. The base model has never seen your FAQ, doesn’t know your 30-day return window, &amp; has no idea that you ship from two warehouses with different lead times. Here’s how each approach closes that gap.</p>
<p><strong>Prompt engineering</strong> means changing the instructions. You write a system prompt — a block of text the user never sees — telling the model who it is, how to behave, what to say &amp; what not to say. For your support bot, that might be: “You are a helpful customer service agent for Acme Store. Be concise &amp; friendly. Our return window is 30 days. Never promise expedited shipping unless the customer has a Premium account. If you don’t know the answer, say so &amp; offer to connect them with a human.” This is free, takes minutes to change, &amp; is where every project should start. It’s like giving someone very detailed instructions before they begin a task. The downside: prompts are fragile. A model update can break behavior that was working yesterday. Different models respond differently to the same prompt. And there’s a ceiling — you can shape tone &amp; set guardrails, but you can’t teach the model facts it doesn’t have through instructions alone. Your prompt can say “our return window is 30 days,” but what about the other 500 policy details customers ask about? You’d run out of context space long before you ran out of policies.</p>
<p><strong>RAG — retrieval-augmented generation</strong> — means giving the model your data at query time. Instead of stuffing every possible fact into the prompt upfront, you search your own documents for the pieces relevant to each specific question &amp; inject them into the context alongside it. A customer asks “can I return shoes I wore outside?” Your system converts that question into an embedding — a numerical fingerprint, covered in article 00d — searches your knowledge base for semantically similar content, finds the relevant section of your returns policy, &amp; pastes it into the prompt. The model answers using your actual documentation, grounded in your facts. Think of it as handing someone a reference book open to the right page before they answer a question. RAG costs more per query — you’re paying for embedding, retrieval, &amp; a longer prompt — but it dramatically reduces hallucination on domain-specific questions. Your bot stops inventing return policies &amp; starts citing real ones. The downside: retrieval quality matters enormously. If your system pulls the wrong documents — the shipping policy instead of the returns policy — the model gives a confidently wrong answer grounded in irrelevant context. Bad retrieval is worse than no retrieval, cuz it looks authoritative.</p>
<p><strong>Fine-tuning</strong> means retraining the model on your data. You take a pre-trained model &amp; run additional training on your own examples — hundreds or thousands of ideal customer interactions, your specific tone, your classification labels, your domain terminology. This bakes behavior into the model’s weights rather than relying on instructions or context. For your support bot, you might fine-tune on a year’s worth of your best human agents’ conversations, so the model learns to respond the way your team actually talks — the specific phrases, the escalation patterns, the way they handle frustrated customers. Think of it as sending someone to a specialized apprenticeship in your support org. It’s expensive — $100 to $10K or more — takes days to weeks, creates a model version you need to maintain, &amp; couples you to a specific base model. But for tasks where consistent specialized behavior matters — medical terminology, legal analysis, a very particular output format — it’s sometimes the only way to get reliability.</p>
<h2 id="the-decision-framework">The decision framework</h2>
<p>Start with prompts. Almost always. Add RAG when the model needs access to your data to answer well — when the facts it needs aren’t in its training data &amp; won’t fit in a static prompt. Fine-tune only when prompts &amp; RAG together can’t get you to the quality bar, &amp; the use case justifies the investment. For most support bots, prompt engineering plus RAG gets you 90% of the way there. Fine-tuning is the last 10%, &amp; it comes with ongoing maintenance cost every time the base model updates or your business changes.</p>
<p>The most common mistake is jumping straight to fine-tuning cuz it sounds like “making the AI smarter.” Teams spend weeks &amp; thousands of dollars on fine-tuning when what they actually needed was a better prompt or better retrieval. Fine-tuning is expensive, slow, creates maintenance burden, &amp; the quality gains are often smaller than expected — esp when the real problem was that the model didn’t have access to the right information, which is a retrieval problem, not a training problem. Get the layering right &amp; you’ll ship faster, spend less, &amp; keep the flexibility to swap models when a better one drops.</p><div class="end-mark">╌╌ end ╌╌</div>
  </div>
  <nav class="article-nav">
  <a href="04-00d-embeddings-and-the-shape-of-meaning.html">&larr; Embeddings & the Shape of Meaning</a>
  <a class="nav-index" href="../index.html">Index</a>
  <a href="06-00f-the-llm-landscape.html">The LLM Landscape &rarr;</a>
</nav>
  
</body>
</html>